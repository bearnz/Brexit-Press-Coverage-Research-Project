{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DATA301 project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO7dHhWcbPjVywkcMD4ns1F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raebeht/DATA301-Research-Project/blob/master/DATA301_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLtFL8aymDue",
        "colab_type": "text"
      },
      "source": [
        "Author: Josh Smith\n",
        "\n",
        "This is a python program that utilises pySpark and the GDELT event project to answer the following research question: How did the global perception of the UK’s relationship with the EU, compared to the UK’s own perception, change following the Brexit vote in 2016? \n",
        "\n",
        "The results are measured on the Goldstein Scale of each event and the tone score of each source article that is analysed. Each article has its contents scraped and word frequency processed, with terms connected with brexit qualifying a source for entry into the overall analysis.\n",
        "\n",
        "The Goldstein Scale is measured on a scale of -10 to +10 that captures the likely impact that type of event will have on the political stability of a country.\n",
        "\n",
        "The Tone of a source covering an event is measured on a scale of -100 (negative) to +100 (positive)\n",
        "\n",
        "Note: The processing time of this program will take over an hour if the scope of the data processing is equal-to or more-than 7 days for each data-point (currently 3 data points). Future versions utilising google cloud and more refined parallelism will hopefully cut this time down. \n",
        "\n",
        "Results from time of project submission are at the bottom of this page, re-running the program in it's current state will take >1 hour and may produce different results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3yOp26ZyjVQ",
        "colab_type": "text"
      },
      "source": [
        "Timer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5DsTDhkyhAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "start_time = time.perf_counter()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_SuWdDnNkJe",
        "colab_type": "text"
      },
      "source": [
        "Libraries setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evXZu6fDNM0u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "a49749c0-9268-4d74-cfd3-b820c2f30e48"
      },
      "source": [
        "#library and code setup\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!pip install -q pyspark\n",
        "!pip install gdelt\n",
        "import pyspark, os\n",
        "from pyspark import SparkConf, SparkContext\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python3\"\n",
        "os.environ[\"JAVA_HOME\"]=\"/usr/lib/jvm/java-8-openjdk-amd64/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 204.7MB 66kB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 44.0MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gdelt\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/f9/a3d5111c8f17334b1752c32aedaab0d01ab4324bf26417bd41890d5b25d0/gdelt-0.1.10.6.1-py2.py3-none-any.whl (773kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gdelt) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdelt) (2.23.0)\n",
            "Requirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.6/dist-packages (from gdelt) (1.0.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from gdelt) (2.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdelt) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdelt) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdelt) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdelt) (2020.6.20)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.3->gdelt) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->gdelt) (1.12.0)\n",
            "Installing collected packages: gdelt\n",
            "Successfully installed gdelt-0.1.10.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcO01WFINUjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#start spark local server\n",
        "import sys, os\n",
        "from operator import add\n",
        "import time\n",
        "\n",
        "os.environ[\"PYSPARK_PYTHON\"]=\"python3\"\n",
        "\n",
        "import pyspark\n",
        "from pyspark import SparkConf, SparkContext\n",
        "\n",
        "try:\n",
        "  conf = SparkConf().setMaster(\"local[*]\").set(\"spark.executor.memory\", \"1g\")\n",
        "  sc = SparkContext(conf = conf)\n",
        "except ValueError:\n",
        "  pass\n",
        "\n",
        "def dbg(x):\n",
        "  \"\"\" A helper function to print debugging information on RDDs \"\"\"\n",
        "  if isinstance(x, pyspark.RDD):\n",
        "    print([(t[0], list(t[1]) if \n",
        "            isinstance(t[1], pyspark.resultiterable.ResultIterable) else t[1])\n",
        "           if isinstance(t, tuple) else t\n",
        "           for t in x.take(100)])\n",
        "  else:\n",
        "    print(x)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKBdZ1fVWd-r",
        "colab_type": "text"
      },
      "source": [
        "External code for scraping websites from URLs, sourced from https://gist.github.com/linwoodc3/e12a7fbebfa755e897697165875f8fdb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH1yjGkeXHAp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5576248-5ba6-4a2e-bb26-856e4b2193d5"
      },
      "source": [
        "!pip install requests\n",
        "!pip3 install newspaper3k\n",
        "!pip install bs4\n",
        "!pip install requests\n",
        "!pip install readability-lxml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 2.7MB/s \n",
            "\u001b[?25hCollecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 8.2MB/s \n",
            "\u001b[?25hCollecting feedparser>=5.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 29.6MB/s \n",
            "\u001b[?25hCollecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.8.1)\n",
            "Collecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/0e/9ab599d6e78f0340bb1d1e28ddeacb38c8bb7f91a1b0eae9a24e9603782f/tldextract-2.2.2-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.23.0)\n",
            "Collecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (7.0.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.3->newspaper3k) (1.12.0)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (47.3.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Building wheels for collected packages: jieba3k, feedparser, feedfinder2, tinysegmenter\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp36-none-any.whl size=7398406 sha256=9f53b1e7dedd090d7d83662c3d2f7c693d11f77e81967c56419bf23c709d0ace\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for feedparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedparser: filename=feedparser-5.2.1-cp36-none-any.whl size=44940 sha256=669de377e7ce1512dc3832446adfbf9ee878e6812a5e5f9c4fab1ded04a39397\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp36-none-any.whl size=3357 sha256=5ffad5080070c49d35be6bab5826b751bf6900c97007cae80b4fe80b52085216\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp36-none-any.whl size=13539 sha256=c46c521aa0cd6d41ba955246b3514d5f78538731011183a520d8f315486d317a\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "Successfully built jieba3k feedparser feedfinder2 tinysegmenter\n",
            "Installing collected packages: jieba3k, feedparser, cssselect, requests-file, tldextract, feedfinder2, tinysegmenter, newspaper3k\n",
            "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-5.2.1 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 tinysegmenter-0.3 tldextract-2.2.2\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.6.20)\n",
            "Collecting readability-lxml\n",
            "  Downloading https://files.pythonhosted.org/packages/39/a6/cfe22aaa19ac69b97d127043a76a5bbcb0ef24f3a0b22793c46608190caa/readability_lxml-0.8.1-py3-none-any.whl\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.6/dist-packages (from readability-lxml) (3.0.4)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.6/dist-packages (from readability-lxml) (1.1.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from readability-lxml) (4.2.6)\n",
            "Installing collected packages: readability-lxml\n",
            "Successfully installed readability-lxml-0.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQAK9YK0zjZ6",
        "colab_type": "text"
      },
      "source": [
        "Text scraper from URL - Slightly modifed from source to attain consistent syntax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_v8oxGXWdNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Author: Linwood Creekmore\n",
        "# Email: valinvescap@gmail.com\n",
        "# Description:  Python script to pull content from a website (works on news stories).\n",
        "\n",
        "#Licensed under GNU GPLv3; see https://choosealicense.com/licenses/lgpl-3.0/ for details\n",
        "\n",
        "# Notes\n",
        "\"\"\"\n",
        "23 Oct 2017: updated to include readability based on PyCon talk: https://github.com/DistrictDataLabs/PyCon2016/blob/master/notebooks/tutorial/Working%20with%20Text%20Corpora.ipynb\n",
        "18 Jul 2018: added keywords and summary\n",
        "\"\"\"\n",
        "\n",
        "###################################\n",
        "# Standard Library imports\n",
        "###################################\n",
        "\n",
        "import re\n",
        "import pytz\n",
        "import datetime\n",
        "import platform\n",
        "\n",
        "\n",
        "###################################\n",
        "# Third party imports\n",
        "###################################\n",
        "\n",
        "import requests\n",
        "from newspaper import Article\n",
        "from bs4 import BeautifulSoup\n",
        "from readability.readability import Document as Paper\n",
        "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
        "\n",
        "\n",
        "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
        "\n",
        "\n",
        "done = {}\n",
        "\n",
        "\n",
        "def textgetter(url):\n",
        "    \"\"\"Scrapes web news and returns the content\n",
        "    Parameters\n",
        "    ----------\n",
        "    url : str\n",
        "        web address to news report\n",
        "    Returns \n",
        "    -------\n",
        "    \n",
        "    answer : dict\n",
        "        Python dictionary with key/value pairs for:\n",
        "            text (str) - Full text of article\n",
        "            url (str) - url to article\n",
        "            title (str) - extracted title of article\n",
        "            author (str) - name of extracted author(s)\n",
        "            base (str) - base url of where article was located\n",
        "            provider (str) - string of the news provider from url\n",
        "            published_date (str,isoformat) - extracted date of article\n",
        "            top_image (str) - extracted url of the top image for article\n",
        "    \"\"\"\n",
        "    global done\n",
        "    TAGS = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li']\n",
        "\n",
        "    # regex for url check\n",
        "    s = re.compile('(http://|https://)([A-Za-z0-9_\\.-]+)')\n",
        "    u = re.compile(\"(http://|https://)(www.)?(.*)(\\.[A-Za-z0-9]{1,4})$\")\n",
        "    if s.search(url):\n",
        "        site = u.search(s.search(url).group()).group(3)\n",
        "    else:\n",
        "        site = None\n",
        "    answer = {}\n",
        "    # check that its an url\n",
        "    if s.search(url):\n",
        "        if url in done.keys():\n",
        "            yield done[url]\n",
        "            pass\n",
        "        try:\n",
        "            # make a request to the url\n",
        "            r = requests.get(url, verify=False, timeout=1)\n",
        "        except:\n",
        "            # if the url does not return data, set to empty values\n",
        "            done[url] = \"Unable to reach website.\"\n",
        "            answer['author'] = '_'\n",
        "            answer['base'] = s.search(url).group()\n",
        "            answer['provider']=site\n",
        "            answer['published_date']='_'\n",
        "            answer['text'] = \"Unable to reach website.\"\n",
        "            answer['title'] = '_'\n",
        "            answer['top_image'] = '_'\n",
        "            answer['url'] = url\n",
        "            answer['keywords']='_'\n",
        "            answer['summary']='_'\n",
        "            yield answer\n",
        "        # if url does not return successfully, set ot empty values\n",
        "        if r.status_code != 200:\n",
        "            done[url] = \"Unable to reach website.\"\n",
        "            answer['author'] = '_'\n",
        "            answer['base'] = s.search(url).group()\n",
        "            answer['provider']=site\n",
        "            answer['published_date']='_'\n",
        "            answer['text'] = \"Unable to reach website.\"\n",
        "            answer['title'] = '_'\n",
        "            answer['top_image'] = '_'\n",
        "            answer['url'] = url\n",
        "            answer['keywords']='_'\n",
        "            answer['summary']='_'\n",
        "\n",
        "        # test if length of url content is greater than 500, if so, fill data\n",
        "        if len(r.content)>500:\n",
        "            # set article url\n",
        "            article = Article(url)\n",
        "            # test for python version because of html different parameters\n",
        "            if int(platform.python_version_tuple()[0])==3:\n",
        "                article.download(input_html=r.content)\n",
        "            elif int(platform.python_version_tuple()[0])==2:\n",
        "                article.download(html=r.content)\n",
        "            # parse the url\n",
        "            article.parse()\n",
        "            article.nlp()\n",
        "            # if parse doesn't pull text fill the rest of the data\n",
        "            if len(article.text) >= 200:\n",
        "                answer['author'] = \", \".join(article.authors)\n",
        "                answer['base'] = s.search(url).group()\n",
        "                answer['provider']=site\n",
        "                answer['published_date'] = article.publish_date\n",
        "                answer['keywords']=article.keywords\n",
        "                answer['summary']=article.summary\n",
        "                # convert the data to isoformat; exception for naive date\n",
        "                if isinstance(article.publish_date,datetime.datetime):\n",
        "                    try:\n",
        "                        answer['published_date']=article.publish_date.astimezone(pytz.utc).isoformat()\n",
        "                    except:\n",
        "                        answer['published_date']=article.publish_date.isoformat()\n",
        "                \n",
        "\n",
        "                answer['text'] = article.text\n",
        "                answer['title'] = article.title\n",
        "                answer['top_image'] = article.top_image\n",
        "                answer['url'] = url\n",
        "                \n",
        "                \n",
        "\n",
        "            # if previous didn't work, try another library\n",
        "            else:\n",
        "                doc = Paper(r.content)\n",
        "                data = doc.summary()\n",
        "                title = doc.title()\n",
        "                soup = BeautifulSoup(data, 'lxml')\n",
        "                newstext = \" \".join([l.text for l in soup.find_all(TAGS)])\n",
        "\n",
        "                # as we did above, pull text if it's greater than 200 length\n",
        "                if len(newstext) > 200:\n",
        "                    answer['author'] = '_'\n",
        "                    answer['base'] = s.search(url).group()\n",
        "                    answer['provider']=site\n",
        "                    answer['published_date']='_'\n",
        "                    answer['text'] = newstext\n",
        "                    answer['title'] = title\n",
        "                    answer['top_image'] = '_'\n",
        "                    answer['url'] = url\n",
        "                    answer['keywords']='_'\n",
        "                    answer['summary']='_'\n",
        "                # if nothing works above, use beautiful soup\n",
        "                else:\n",
        "                    newstext = \" \".join([\n",
        "                        l.text\n",
        "                        for l in soup.find_all(\n",
        "                            'div', class_='field-item even')\n",
        "                    ])\n",
        "                    done[url] = newstext\n",
        "                    answer['author'] = '_'\n",
        "                    answer['base'] = s.search(url).group()\n",
        "                    answer['provider']=site\n",
        "                    answer['published_date']='_'\n",
        "                    answer['text'] = newstext\n",
        "                    answer['title'] = title\n",
        "                    answer['top_image'] = '_'\n",
        "                    answer['url'] = url\n",
        "                    answer['keywords']='_'\n",
        "                    answer['summary']='_'\n",
        "        # if nothing works, fill with empty values\n",
        "        else:\n",
        "            answer['author'] = '_'\n",
        "            answer['base'] = s.search(url).group()\n",
        "            answer['provider']=site\n",
        "            answer['published_date']='_'\n",
        "            answer['text'] = 'No text returned'\n",
        "            answer['title'] = '_'\n",
        "            answer['top_image'] = '_'\n",
        "            answer['url'] = url\n",
        "            answer['keywords']='_'\n",
        "            answer['summary']='_'\n",
        "            yield answer\n",
        "        yield answer\n",
        "\n",
        "    # the else clause to catch if invalid url passed in\n",
        "    else:\n",
        "        answer['author'] = '_'\n",
        "        answer['base'] = '_' #s.search(url).group()\n",
        "        answer['provider']=site\n",
        "        answer['published_date']='_'\n",
        "        answer['text'] = 'This is not a proper url'\n",
        "        answer['title'] = '_'\n",
        "        answer['top_image'] = '_'\n",
        "        answer['url'] = url\n",
        "        answer['keywords']='_'\n",
        "        answer['summary']='_'\n",
        "        yield answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DqZnKCGNsM1",
        "colab_type": "text"
      },
      "source": [
        "Fetch GDELT data for the date ranges\n",
        "Note: This will throw errors of dates with no events, these dates get removed after generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amZE8bFhNd0r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9238a20c-d35e-4616-fbde-af3d977817ac"
      },
      "source": [
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from datetime import date, timedelta\n",
        "import pandas as pd\n",
        "import gdelt\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "gd = gdelt.gdelt(version=2)\n",
        "executor = ProcessPoolExecutor()\n",
        "\n",
        "def get_filename(x):\n",
        "  date = x.strftime('%Y%m%d')\n",
        "  return \"{}_gdeltdata.csv\".format(date)\n",
        "\n",
        "def intofile(filename):\n",
        "    try:\n",
        "        if not os.path.exists(filename):\n",
        "          date = filename.split(\"_\")[0]\n",
        "          data = gd.Search(date, table='events',coverage=False)\n",
        "          data.to_csv(filename,encoding='utf-8',index=False)\n",
        "    except:\n",
        "        print(\"Error occurred at\", filename)\n",
        "\n",
        "def get_invalid_dates(filename):\n",
        "    try:\n",
        "        if not os.path.exists(filename):\n",
        "            date = filename.split(\"_\")[0]\n",
        "            gd.Search(date, table='events',coverage=False)\n",
        "    except:\n",
        "      return filename\n",
        "\n",
        "# pull the data from gdelt into multi files, then removes dates that have no data in GDELT\n",
        "bad_data = []\n",
        "\n",
        "dates_7d_before = [get_filename(x) for x in pd.date_range('2016 Jun 17','2016 Jun 23')]\n",
        "for date in dates_7d_before:\n",
        "  if get_invalid_dates(date) is not None:\n",
        "      bad_data.append(get_invalid_dates(date))\n",
        "while len(bad_data) > 0:\n",
        "  date = bad_data.pop()\n",
        "  dates_7d_before.remove(date)\n",
        "\n",
        "dates_7d_after = [get_filename(x) for x in pd.date_range('2016 Jun 19','2016 Jun 25')]\n",
        "for date in dates_7d_after:\n",
        "  if get_invalid_dates(date) is not None:\n",
        "      bad_data.append(get_invalid_dates(date))\n",
        "while len(bad_data) > 0:\n",
        "  date = bad_data.pop()\n",
        "  dates_7d_after.remove(date)\n",
        "\n",
        "dates_2yrs_after = [get_filename(x) for x in pd.date_range('2019 Jun 2','2019 Jun 8')]\n",
        "for date in dates_2yrs_after:\n",
        "  if get_invalid_dates(date) is not None:\n",
        "      bad_data.append(get_invalid_dates(date))\n",
        "while len(bad_data) > 0:\n",
        "  date = bad_data.pop()\n",
        "  dates_2yrs_after.remove(date)\n",
        "\n",
        "results = list(executor.map(intofile,dates_7d_before+dates_7d_after+dates_2yrs_after))\n",
        "# dbg(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVxha3SRN5pd",
        "colab_type": "text"
      },
      "source": [
        "Read data in RDDs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOchGPnbNi6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SQLContext\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "data_7d_before = sqlContext.read.option(\"header\", \"true\").csv(dates_7d_before)\n",
        "data_7d_after = sqlContext.read.option(\"header\", \"true\").csv(dates_7d_after)\n",
        "data_2yrs_after = sqlContext.read.option(\"header\", \"true\").csv(dates_2yrs_after)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kY0b-yNOAxr",
        "colab_type": "text"
      },
      "source": [
        "Event and keyword data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJPuvtNROD6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# events = ['101', '102', '105', '106', '107', '110', '111', '112', '114', '115',\n",
        "#           '116', '120', '121', '122', '123', '124', '125', '126', '127', '128', \n",
        "#           '130', '131', '132', '133', '134', '136', '137', '138', '139', '140', \n",
        "#           '143', '144', '153', '154', '160', '161', '162', '163', '164', '165',\n",
        "#           '166', '170', '171', '172', '173', '174', '175', '180', '181', '182',\n",
        "#           '184', '185', '191', '192', '193', '194', '195', '196', '201', '202',\n",
        "#           '203', '211', '213', '214', '231', '232', '234', '241', '242', '243',\n",
        "#           '244', '252', '253', '254', '255', '256', '311', '312', '313', '314',\n",
        "#           '331', '332', '333', '334', '355', '356', '811', '812', '813', '831', \n",
        "#           '833', '834', '841', '842', '861', '862', '863', '871', '872', '873', \n",
        "#           '874', '1012', '1013', '1014', '1033', '1034', '1041', '1042', '1043',\n",
        "#           '1044', '1051', '1053', '1054', '1056', '1121', '1123', '1124', '1125',\n",
        "#           '1211', '1212', '1213', '1222', '1224', '1231', '1232', '1233', '1241',\n",
        "#           '1243', '1244', '1246', '1311', '1312', '1313', '1322', '1382', '1383',\n",
        "#           '1384', '1385', '1412', '1413', '1414', '1431', '1621', '1623', '1662',\n",
        "#           '1711', '1721', '1722', '1723', '1724', '1821', '1822', '1823', '1831', '1832'] #Didn't end up using these, may look into using these as a filter in future versions\n",
        "\n",
        "keywords = ['backstop', 'brexit', 'brexiteer',\n",
        "            'brexiter', 'brextremist', 'brexshit', 'brextension',\n",
        "            'chequers', 'customs', 'union', 'divorce', 'bill', 'eu',\n",
        "            'exit', 'flextension', 'hard', 'border', 'indicative', 'vote', 'implementation',\n",
        "            'irish', 'leaver', 'lexit', 'meaningful', 'no-deal',\n",
        "            \"people's\", 'declaration',\n",
        "            'remain', 'remainer', 'second', 'referendum', 'slow', 'soft', 'withdrawal', 'agreement']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPKYG6mllvIS",
        "colab_type": "text"
      },
      "source": [
        "Text file of common english words sourced from https://simple.wikipedia.org/wiki/Wikipedia:List_of_1000_basic_words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L48KWkxml0jR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2c3c0b8-d747-432d-8706-494eecbd5535"
      },
      "source": [
        "%%writefile common_words.txt\n",
        "a about above across act active activity add afraid after again age ago agree air all alone along already always am amount an and angry another answer any anyone anything anytime appear apple are area arm army around arrive art as ask at attack aunt autumn away baby back bad bag ball bank base basket bath be bean bear beautiful bed bedroom beer behave before begin behind bell below besides best better between big bird birth birthday bit bite black bleed block blood blow blue board boat body boil bone book border born borrow both bottle bottom bowl box boy branch brave bread break breakfast breathe bridge bright bring brother brown brush build burn business bus busy but buy by cake call can candle cap car card care careful careless carry case cat catch central century certain chair chance change chase cheap cheese chicken child children chocolate choice choose circle city class clever clean clear climb clock cloth clothes cloud cloudy close coffee coat coin cold collect colour comb comfortable common compare come complete computer condition continue control cook cool copper corn corner correct cost contain count country course cover crash cross cry cup cupboard cut dance dangerous dark daughter day dead decide decrease deep deer depend desk destroy develop die different difficult dinner direction dirty discover dish do dog door double down draw dream dress drink drive drop dry duck dust duty each ear early earn earth east easy eat education effect egg eight either electric elephant else empty end enemy enjoy enough enter equal entrance escape even evening event ever every everyone exact everybody examination example except excited exercise expect expensive explain extremely eye face fact fail fall false family famous far farm father fast fat fault fear feed feel female fever few fight fill film find fine finger finish fire first fish fit five fix flag flat float floor flour flower fly fold food fool foot football for force foreign forest forget forgive fork form fox four free freedom freeze fresh friend friendly from front fruit full fun funny furniture further future game garden gate general gentleman get gift give glad glass go goat god gold good goodbye grandfather grandmother grass grave great green gray ground group grow gun hair half hall hammer hand happen happy hat hate have he head healthy hear heavy heart heaven height hello help hen her here hers hide high hill him his hit hobby hold hole holiday home hope horse hospital hot hotel house how hundred hungry hour hurry husband hurt i ice idea if important in increase inside into introduce invent iron invite is island it its\n",
        "jelly job join juice jump just keep key kill kind king kitchen knee knife knock know ladder lady lamp land large last late lately laugh lazy lead leaf learn leg left lend length less lesson let letter library lie life light like lion lip list listen little live lock lonely long look lose lot love low lower luck\n",
        "machine main make male man many map mark market marry matter may me meal mean measure meat medicine meet member mention method middle milk million mind minute miss mistake mix model modern moment money monkey month moon more morning most mother mountain mouth move much music must my name narrow nation nature near nearly neck need needle neighbour neither net never new news newspaper next nice night nine no noble noise none nor north nose not nothing notice now number obey object ocean of off offer office often oil old on one only open opposite or orange order other our out outside over own page pain paint pair pan paper parent park part partner party pass past path pay peace pen pencil people pepper per perfect period person petrol photograph piano pick picture piece pig pin pink place plane plant plastic plate play please pleased plenty pocket point poison police polite pool poor popular position possible potato pour power present press pretty prevent price prince prison private prize probably problem produce promise proper protect provide public pull punish pupil push put queen question quick quiet quite radio rain rainy raise reach read ready real really receive record red remember remind remove rent repair repeat reply report rest restaurant result return rice rich ride right ring rise road rob rock room round rubber rude rule ruler run rush sad safe sail salt same sand save say school science scissors search seat see seem sell send sentence serve seven several sex shade shadow shake shape share sharp she sheep sheet shelf shine ship shirt shoe shoot shop short should shoulder shout show sick side signal silence silly silver similar simple single since sing sink sister sit six size skill skin skirt sky sleep slip small smell smile smoke snow so soap sock soft some someone something sometimes son soon sorry sound soup south space speak special speed spell spend spoon sport spread spring square stamp stand star start station stay steal steam step still stomach stone stop store storm story strange street strong structure student study stupid subject substance successful such sudden sugar suitable summer sun sunny support sure surprise sweet swim sword table take talk tall taste taxi tea teach team tear telephone television tell ten tennis terrible test than that the their then there therefore these thick thin thing think third this though threat three tidy tie title to today toe together tomorrow tonight too tool tooth top total touch town train tram travel tree trouble true trust twice try turn type ugly uncle under understand unit until up use useful usual usually vegetable very village voice visit wait wake walk want warm was wash waste watch water way we weak wear weather wedding week weight welcome were well west wet what wheel when where which while white who why wide wife wild will win wind window wine winter wire wise wish with without woman wonder word work world worry\n",
        "yard yell yesterday yet you young your zero zoo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing common_words.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiRovwEEOXxo",
        "colab_type": "text"
      },
      "source": [
        "Code to answer research question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "249ib42ROStg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from math import sqrt, log\n",
        "from statistics import mean, median, stdev, variance\n",
        "\n",
        "def get_source_country(data):\n",
        "  country_sources = data.rdd.map(lambda row: ((row['GLOBALEVENTID'], row['Actor1CountryCode'], row['Actor2CountryCode'], (row['SOURCEURL'].split('/'))[2].split('.')[-1], row['SOURCEURL'], row['EventCode'], row['GoldsteinScale'], row['AvgTone']), 1))\n",
        "  country_sources = country_sources.filter(lambda line: line[0][4] is not None)\n",
        "  country_sources = country_sources.filter(lambda line: 'GBR' in [line[0][1], line[0][2]])\n",
        "  return country_sources\n",
        "\n",
        "def get_word_counts(article):\n",
        "  text = sc.parallelize(article.split(' '))\n",
        "  words = text.flatMap(lambda line: [(word.lower(), 1) for word in line.split(\" \")])\n",
        "  counts = (words.reduceByKey(lambda a, b: a+b).sortBy(lambda x: x[1], False))\n",
        "  return counts\n",
        "\n",
        "def scrape_data(sources):\n",
        "  data = sources.map(lambda row: row)\n",
        "  scraped_data = data.map(lambda row: [row[0][0], next(textgetter(row[0][4]))])\n",
        "  scraped_data = scraped_data.filter(lambda row: type(row[1]) is dict)\n",
        "  scraped_text = scraped_data.map(lambda row: row[1]['text'])\n",
        "  scraped_text = scraped_text.filter(lambda row: row.lower() is not 'no text returned')\n",
        "  return scraped_text, scraped_data\n",
        "\n",
        "def calc_IDFi(sources):\n",
        "  text, scraped_data = scrape_data(sources)\n",
        "  #Monotonisation\n",
        "  text = [x for x in text.toLocalIterator()]\n",
        "  N = len(text)\n",
        "  words_scraped = scraped_data.map(lambda row: row[1]['text']).filter(lambda row: row.lower() is not 'no text returned')\n",
        "  word_counts = words_scraped.flatMap(lambda line: [(word.lower(), 1) for word in line.split(\" \")]).reduceByKey(lambda a, b: a+b).sortBy(lambda x: x[1], False)\n",
        "  word_counts = word_counts.subtractByKey(common)\n",
        "  IDFi = word_counts.groupByKey().map(lambda ind: (ind[0], len(ind[1]))).sortBy(lambda ind: ind[1], True).map(lambda x: (x[0], (log(N/x[1], 2))))\n",
        "  return IDFi, scraped_data\n",
        "\n",
        "def calc_TFij(s_data):\n",
        "  words_scraped = s_data.map(lambda row: [row[0], row[1]['text']])\n",
        "  words_scraped = words_scraped.filter(lambda row: row[1].lower() is not 'no text returned')\n",
        "  word_counts = words_scraped.map(lambda line: [line[0], [(word.lower(), 1) for word in line[1].split(\" \")]])\n",
        "  #Monotonisation\n",
        "  articles = [x for x in word_counts.toLocalIterator()]\n",
        "  TFij = []\n",
        "  for article in articles:\n",
        "    ID = article[0]\n",
        "    article = article[1]\n",
        "    #This has had to be monotonised because I couldn't get the mapping to work without the program crashing\n",
        "    par_data = sc.parallelize(article)\n",
        "    par_data = par_data.reduceByKey(lambda a, b: a+b).sortBy(lambda x: x[1], False).subtractByKey(common)\n",
        "    max_value = par_data.first()[1]\n",
        "    par_data = par_data.map(lambda word: [word[0], word[1]/max_value])\n",
        "    TFij.append([ID, [x for x in par_data.toLocalIterator()]])\n",
        "  return TFij\n",
        "\n",
        "def calc_TFijxIDFi(sources):\n",
        "  IDFi, s_data = calc_IDFi(sources)\n",
        "  TFij = calc_TFij(s_data)\n",
        "  #Monotonisation\n",
        "  glo_TFIDF = []\n",
        "  for ind in range(len(TFij)):\n",
        "    #Monotoisation to avoid crashing\n",
        "    ID = TFij[ind][0]\n",
        "    TFij[ind] = TFij[ind][1]\n",
        "    TF_art = sc.parallelize(TFij[ind])\n",
        "    if TF_art.count() > 5:\n",
        "      TFijxIDFi = TF_art.join(IDFi)\n",
        "      TFijxIDFi = TFijxIDFi.map(lambda word: (word[0], word[1][0]*word[1][1]))\n",
        "      TFijxIDFi = TFijxIDFi.sortBy(lambda word: word[1], False)\n",
        "      glo_TFIDF.append([ID, [x for x in TFijxIDFi.toLocalIterator()]])\n",
        "  return glo_TFIDF\n",
        "\n",
        "def get_relevant_sources(TFIDF):\n",
        "  #Monotonisation\n",
        "  checked_sources = []\n",
        "  for source in TFIDF:\n",
        "    #Monotoisation to avoid crashing\n",
        "    ID = source[0]\n",
        "    source = sc.parallelize(source[1])\n",
        "    data = source.take(100)\n",
        "    source = source.filter(lambda item: any(item[0] in data for word in keywords))\n",
        "    checked_sources.append([ID, [x for x in source.toLocalIterator()]])\n",
        "  return checked_sources\n",
        "\n",
        "common = sc.textFile(\"common_words.txt\").flatMap(lambda line: [(word, 1) for word in line.split(\" \")])\n",
        "\n",
        "sources_7d_before = get_source_country(data_7d_before)\n",
        "TFijxIDFi_7d_before = calc_TFijxIDFi(sources_7d_before)\n",
        "rel_srcs_7d_before = get_relevant_sources(TFijxIDFi_7d_before)\n",
        "\n",
        "sources_7d_after = get_source_country(data_7d_after)\n",
        "TFijxIDFi_7d_after = calc_TFijxIDFi(sources_7d_after)\n",
        "rel_srcs_7d_after = get_relevant_sources(TFijxIDFi_7d_after)\n",
        "\n",
        "sources_2yrs_after = get_source_country(data_2yrs_after)\n",
        "TFijxIDFi_2yrs_after = calc_TFijxIDFi(sources_2yrs_after)\n",
        "rel_srcs_2yrs_after = get_relevant_sources(TFijxIDFi_2yrs_after)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gz94DS5BsBX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_uk_non_uk(IDs, sources):\n",
        "  data = sources.map(lambda row: [row[0][0], [row[0][7], row[0][6], row[0][3]]]) #EventID, Tone, GsS, URL suffix\n",
        "  IDs = sc.parallelize(IDs)\n",
        "  global_map = IDs.join(data)\n",
        "  uk_sources = global_map.map(lambda row: [row[0], row[1][1]]).filter(lambda row: row[1][2] == 'uk')\n",
        "  non_uk_sources = global_map.map(lambda row: [row[0], row[1][1]]).filter(lambda row: row[1][2] != 'uk')\n",
        "  return uk_sources, non_uk_sources\n",
        "  \n",
        "def get_stats(rdd1):\n",
        "  spread_tone = stdev([float(x[1][0]) for x in rdd1.toLocalIterator()])\n",
        "  spread_GsS = stdev([float(x[1][1]) for x in rdd1.toLocalIterator()])\n",
        "  median_tone = median([float(x[1][0]) for x in rdd1.toLocalIterator()])\n",
        "  median_GsS = median([float(x[1][1]) for x in rdd1.toLocalIterator()])\n",
        "  mean_tone = mean([float(x[1][0]) for x in rdd1.toLocalIterator()])\n",
        "  mean_GsS = mean([float(x[1][1]) for x in rdd1.toLocalIterator()])\n",
        "  return spread_tone, spread_GsS, median_tone, median_GsS, mean_tone, mean_GsS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKsAl4Nihgbp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "235f4d11-67f4-42bc-f9cf-34ad21a06264"
      },
      "source": [
        "uk_7d_before, non_uk_7d_before = split_uk_non_uk(rel_srcs_7d_before, sources_7d_before)\n",
        "spread_tone_uk_7d_before, spread_GsS_uk_7d_before, median_tone_uk_7d_before, median_GsS_uk_7d_before, mean_tone_uk_7d_before, mean_GsS_uk_7d_before = get_stats(uk_7d_before)\n",
        "spread_tone_non_uk_7d_before, spread_GsS_non_uk_7d_before, median_tone_non_uk_7d_before, median_GsS_non_uk_7d_before, mean_tone_non_uk_7d_before, mean_GsS_non_uk_7d_before = get_stats(non_uk_7d_before)\n",
        "print(\"7 days pre-referendum\")\n",
        "print(\"UK-internal statistics:\")\n",
        "print(\"Tone of articles:\")\n",
        "print(\"Standard Deviation:\", spread_tone_uk_7d_before)\n",
        "print(\"Median:\", median_tone_uk_7d_before)\n",
        "print(\"Mean:\", mean_tone_uk_7d_before)\n",
        "print()\n",
        "print(\"Goldstein Scale of the event:\")\n",
        "print(\"Standard Deviation:\", spread_GsS_uk_7d_before)\n",
        "print(\"Median:\", median_GsS_uk_7d_before)\n",
        "print(\"Mean:\", mean_GsS_uk_7d_before)\n",
        "print()\n",
        "print(\"UK-external statistics:\")\n",
        "print(\"Tone of articles:\")\n",
        "print(\"Standard Deviation:\", spread_tone_non_uk_7d_before)\n",
        "print(\"Median:\", median_tone_non_uk_7d_before)\n",
        "print(\"Mean:\", mean_tone_non_uk_7d_before)\n",
        "print()\n",
        "print(\"Goldstein Scale of the event:\")\n",
        "print(\"Standard Deviation:\", spread_GsS_non_uk_7d_before)\n",
        "print(\"Median:\", median_GsS_non_uk_7d_before)\n",
        "print(\"Mean:\", mean_GsS_non_uk_7d_before)\n",
        "print()\n",
        "uk_7d_after, non_uk_7d_after = split_uk_non_uk(rel_srcs_7d_after, sources_7d_after)\n",
        "spread_tone_uk_7d_after, spread_GsS_uk_7d_after, median_tone_uk_7d_after, median_GsS_uk_7d_after, mean_tone_uk_7d_after, mean_GsS_uk_7d_after = get_stats(uk_7d_after)\n",
        "spread_tone_non_uk_7d_after, spread_GsS_non_uk_7d_after, median_tone_non_uk_7d_after, median_GsS_non_uk_7d_after, mean_tone_non_uk_7d_after, mean_GsS_non_uk_7d_after = get_stats(non_uk_7d_after)\n",
        "print(\"7 days post-referendum\")\n",
        "print(\"UK-internal statistics:\")\n",
        "print(\"Tone of articles:\")\n",
        "print(\"Standard Deviationead:\", spread_tone_uk_7d_after)\n",
        "print(\"Median:\", median_tone_uk_7d_after)\n",
        "print(\"Mean:\", mean_tone_uk_7d_after)\n",
        "print()\n",
        "print(\"Goldstein Scale of the event:\")\n",
        "print(\"Standard Deviation:\", spread_GsS_uk_7d_after)\n",
        "print(\"Median:\", median_GsS_uk_7d_after)\n",
        "print(\"Mean:\", mean_GsS_uk_7d_after)\n",
        "print()\n",
        "print(\"UK-external statistics:\")\n",
        "print(\"Tone of articles:\")\n",
        "print(\"Standard Deviation:\", spread_tone_non_uk_7d_after)\n",
        "print(\"Median:\", median_tone_non_uk_7d_after)\n",
        "print(\"Mean:\", mean_tone_non_uk_7d_after)\n",
        "print()\n",
        "print(\"Goldstein Scale of the event:\")\n",
        "print(\"Standard Deviation:\", spread_GsS_non_uk_7d_after)\n",
        "print(\"Median:\", median_GsS_non_uk_7d_after)\n",
        "print(\"Mean:\", mean_GsS_non_uk_7d_after)\n",
        "print()\n",
        "uk_2yrs_after, non_uk_2yrs_after = split_uk_non_uk(rel_srcs_2yrs_after, sources_2yrs_after)\n",
        "spread_tone_uk_2yrs_after, spread_GsS_uk_2yrs_after, median_tone_uk_2yrs_after, median_GsS_uk_2yrs_after, mean_tone_uk_2yrs_after, mean_GsS_uk_2yrs_after = get_stats(uk_2yrs_after)\n",
        "spread_tone_non_uk_2yrs_after, spread_GsS_non_uk_2yrs_after, median_tone_non_uk_2yrs_after, median_GsS_non_uk_2yrs_after, mean_tone_non_uk_2yrs_after, mean_GsS_non_uk_2yrs_after = get_stats(non_uk_2yrs_after)\n",
        "print(\"2 years post-referendum\")\n",
        "print(\"UK-internal statistics:\")\n",
        "print(\"Tone of articles:\")\n",
        "print(\"Standard Deviation:\", spread_tone_uk_2yrs_after)\n",
        "print(\"Median:\", median_tone_uk_2yrs_after)\n",
        "print(\"Mean:\", mean_tone_uk_2yrs_after)\n",
        "print()\n",
        "print(\"Goldstein Scale of the event:\")\n",
        "print(\"Standard Deviation:\", spread_GsS_uk_2yrs_after)\n",
        "print(\"Median:\", median_GsS_uk_2yrs_after)\n",
        "print(\"Mean:\", mean_GsS_uk_2yrs_after)\n",
        "print()\n",
        "print(\"UK-external statistics:\")\n",
        "print(\"Tone of articles:\")\n",
        "print(\"Standard Deviation:\", spread_tone_non_uk_2yrs_after)\n",
        "print(\"Median:\", median_tone_non_uk_2yrs_after)\n",
        "print(\"Mean:\", mean_tone_non_uk_2yrs_after)\n",
        "print()\n",
        "print(\"Goldstein Scale of the event:\")\n",
        "print(\"Standard Deviation:\", spread_GsS_non_uk_2yrs_after)\n",
        "print(\"Median:\", median_GsS_non_uk_2yrs_after)\n",
        "print(\"Mean:\", mean_GsS_non_uk_2yrs_after)\n",
        "\n",
        "end_time = time.perf_counter()\n",
        "print()\n",
        "print(\"Time elapsed:\", end_time-start_time, \"seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7 days pre-referendum\n",
            "UK-internal statistics:\n",
            "Tone of articles:\n",
            "Standard Deviation: 1.7470287573543113\n",
            "Median: -0.32786885245902003\n",
            "Mean: -1.0406219730449306\n",
            "\n",
            "Goldstein Scale of the event:\n",
            "Standard Deviation: 2.7729990641043876\n",
            "Median: 1.0\n",
            "Mean: 1.5333333333333334\n",
            "\n",
            "UK-external statistics:\n",
            "Tone of articles:\n",
            "Standard Deviation: 2.719328055462484\n",
            "Median: -0.867827447519135\n",
            "Mean: -1.457123810807092\n",
            "\n",
            "Goldstein Scale of the event:\n",
            "Standard Deviation: 4.772018697740349\n",
            "Median: 1.9\n",
            "Mean: 1.0204761904761905\n",
            "\n",
            "7 days post-referendum\n",
            "UK-internal statistics:\n",
            "Tone of articles:\n",
            "Standard Deviationead: 2.725712730384123\n",
            "Median: -0.5221932114882499\n",
            "Mean: -2.1541859535737675\n",
            "\n",
            "Goldstein Scale of the event:\n",
            "Standard Deviation: 2.9391253224880227\n",
            "Median: 0.7\n",
            "Mean: 0.934375\n",
            "\n",
            "UK-external statistics:\n",
            "Tone of articles:\n",
            "Standard Deviation: 2.44143811608839\n",
            "Median: -1.13895216400911\n",
            "Mean: -1.5637635275805966\n",
            "\n",
            "Goldstein Scale of the event:\n",
            "Standard Deviation: 4.319980870275943\n",
            "Median: 1.9\n",
            "Mean: 1.2025751072961373\n",
            "\n",
            "2 years post-referendum\n",
            "UK-internal statistics:\n",
            "Tone of articles:\n",
            "Standard Deviation: 3.979008447631385\n",
            "Median: -0.37453183520599\n",
            "Mean: -0.7533156627843367\n",
            "\n",
            "Goldstein Scale of the event:\n",
            "Standard Deviation: 3.3400469363133527\n",
            "Median: 1.9\n",
            "Mean: 1.4236111111111112\n",
            "\n",
            "UK-external statistics:\n",
            "Tone of articles:\n",
            "Standard Deviation: 3.380573559966164\n",
            "Median: -0.0744047619047601\n",
            "Mean: -1.0650188056020096\n",
            "\n",
            "Goldstein Scale of the event:\n",
            "Standard Deviation: 4.121160348450035\n",
            "Median: 2.8\n",
            "Mean: 1.6532894736842105\n",
            "\n",
            "Time elapsed: 8652.001686164 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
